{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea6b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    print(\"Error: GITHUB_TOKEN is not set in .env file\")\n",
    "    exit(1)\n",
    "\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# 测试模型列表\n",
    "BASE_URL = \"https://models.github.ai/inference\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('GITHUB_TOKEN')}\",\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "}\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "MODEL_ID = \"openai/gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d97bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: 为什么鸡过马路？\n",
      "\n",
      "因为它想去对面看看“鸡”是什么样的！\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=GITHUB_TOKEN, base_url=BASE_URL)\n",
    "\n",
    "# 调用 LLM\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"用中文讲个冷笑话\"}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    print(\"LLM response:\", response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"LLM error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio to allow async in Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "client = AsyncOpenAI(api_key=GITHUB_TOKEN, base_url=BASE_URL)\n",
    "\n",
    "# Define a list of prompts for batch processing\n",
    "prompts = [\n",
    "    \"用中文讲一个冷笑话\",\n",
    "    \"再讲一个冷笑话，要跟动物有关\",\n",
    "    \"讲一个跟程序员有关的冷笑话\",\n",
    "    \"讲一个简短的冷笑话\",\n",
    "]\n",
    "\n",
    "# Asynchronous function to call LLM for a single prompt\n",
    "async def call_llm(prompt):\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        return {\"prompt\": prompt, \"response\": response.choices[0].message.content}\n",
    "    except Exception as e:\n",
    "        return {\"prompt\": prompt, \"response\": None, \"error\": str(e)}\n",
    "\n",
    "\n",
    "# Run batch calls asynchronously\n",
    "async def main():\n",
    "    tasks = [call_llm(prompt) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    for result in results:\n",
    "        print(f\"Prompt: {result['prompt']}\")\n",
    "        if result.get(\"response\"):\n",
    "            print(f\"Response: {result['response']}\\n\")\n",
    "        else:\n",
    "            print(f\"Error: {result['error']}\\n\")\n",
    "    return results\n",
    "\n",
    "# Execute async batch calls in Jupyter\n",
    "results = asyncio.run(main())\n",
    "print(\"Batch results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab77aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=GITHUB_TOKEN,\n",
    "    model=MODEL_ID,\n",
    "    base_url=BASE_URL,\n",
    "    temperature=0.2,\n",
    ")\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Define prompts for batch processing\n",
    "prompts = [\n",
    "   \"用中文讲一个幽默的笑话\",\n",
    "    \"讲一个关于动物的幽默笑话\",\n",
    "    \"讲一个与程序员相关的幽默笑话\",\n",
    "    \"讲一个简短的幽默笑话\",\n",
    "]\n",
    "\n",
    "# Asynchronous batch calling with LCEL\n",
    "async def main():\n",
    "    results = []\n",
    "    try:\n",
    "        responses = await chain.abatch([{\"input\": prompt} for prompt in prompts])\n",
    "        results = [{\"prompt\": prompt, \"response\": response} for prompt, response in zip(prompts, responses)]\n",
    "        for result in results:\n",
    "            print(f\"Prompt: {result['prompt']}\")\n",
    "            print(f\"Response: {result['response']}\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"Batch error:\", str(e))\n",
    "        results.append({\"prompt\": \"batch\", \"response\": None, \"error\": str(e)})\n",
    "    return results\n",
    "\n",
    "# Run async batch in Jupyter\n",
    "results = asyncio.run(main())\n",
    "print(\"Batch results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
