{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea6b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    print(\"Error: GITHUB_TOKEN is not set in .env file\")\n",
    "    exit(1)\n",
    "\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# 测试模型列表\n",
    "BASE_URL = \"https://models.github.ai/inference\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('GITHUB_TOKEN')}\",\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "}\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "MODEL_ID = \"openai/gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d97bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: 有一天，一只鸡走进了图书馆，走到图书馆员面前说：“咕咕。”图书馆员给了它三本书。鸡拿着书走了出去。\n",
      "\n",
      "第二天，鸡又来了，还是说：“咕咕。”图书馆员又给了它三本书。鸡又走了。\n",
      "\n",
      "第三天，鸡再次来到图书馆，还是说：“咕咕。”图书馆员这次好奇了，决定跟着它。于是他悄悄跟在鸡后面。\n",
      "\n",
      "鸡走到一个池塘边，把书递给了一只青蛙。青蛙看了看书，摇了摇头，说：“呱，不是这个，不是这个。” \n",
      "\n",
      "图书馆员恍然大悟，原来鸡是给青蛙借书呢！\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=GITHUB_TOKEN, base_url=BASE_URL)\n",
    "\n",
    "# 调用 LLM\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"用中文讲个冷笑话\"}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    print(\"LLM response:\", response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"LLM error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683df83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 用中文讲一个冷笑话\n",
      "Response: 为什么海洋总是那么蓝？\n",
      "\n",
      "因为鱼在里面游泳的时候，看到水就会说：“哇，真蓝！”\n",
      "\n",
      "Prompt: 再讲一个冷笑话，要跟动物有关\n",
      "Error: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'high'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "\n",
      "Prompt: 讲一个跟程序员有关的冷笑话\n",
      "Response: 为什么程序员总是喜欢在海边工作？\n",
      "\n",
      "因为那里有很多“海量数据”！\n",
      "\n",
      "Prompt: 讲一个简短的冷笑话\n",
      "Response: 为什么海洋总是很冷？\n",
      "\n",
      "因为它有很多“海水”（寒水）！\n",
      "\n",
      "Batch results: [{'prompt': '用中文讲一个冷笑话', 'response': '为什么海洋总是那么蓝？\\n\\n因为鱼在里面游泳的时候，看到水就会说：“哇，真蓝！”'}, {'prompt': '再讲一个冷笑话，要跟动物有关', 'response': None, 'error': 'Error code: 400 - {\\'error\\': {\\'message\\': \"The response was filtered due to the prompt triggering Azure OpenAI\\'s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", \\'type\\': None, \\'param\\': \\'prompt\\', \\'code\\': \\'content_filter\\', \\'status\\': 400, \\'innererror\\': {\\'code\\': \\'ResponsibleAIPolicyViolation\\', \\'content_filter_result\\': {\\'hate\\': {\\'filtered\\': False, \\'severity\\': \\'safe\\'}, \\'jailbreak\\': {\\'filtered\\': False, \\'detected\\': False}, \\'self_harm\\': {\\'filtered\\': False, \\'severity\\': \\'safe\\'}, \\'sexual\\': {\\'filtered\\': True, \\'severity\\': \\'high\\'}, \\'violence\\': {\\'filtered\\': False, \\'severity\\': \\'safe\\'}}}}}'}, {'prompt': '讲一个跟程序员有关的冷笑话', 'response': '为什么程序员总是喜欢在海边工作？\\n\\n因为那里有很多“海量数据”！'}, {'prompt': '讲一个简短的冷笑话', 'response': '为什么海洋总是很冷？\\n\\n因为它有很多“海水”（寒水）！'}]\n"
     ]
    }
   ],
   "source": [
    "# Apply nest_asyncio to allow async in Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "client = AsyncOpenAI(api_key=GITHUB_TOKEN, base_url=BASE_URL)\n",
    "\n",
    "# Define a list of prompts for batch processing\n",
    "prompts = [\n",
    "    \"用中文讲一个冷笑话\",\n",
    "    \"再讲一个冷笑话，要跟动物有关\",\n",
    "    \"讲一个跟程序员有关的冷笑话\",\n",
    "    \"讲一个简短的冷笑话\",\n",
    "]\n",
    "\n",
    "# Asynchronous function to call LLM for a single prompt\n",
    "async def call_llm(prompt):\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        return {\"prompt\": prompt, \"response\": response.choices[0].message.content}\n",
    "    except Exception as e:\n",
    "        return {\"prompt\": prompt, \"response\": None, \"error\": str(e)}\n",
    "\n",
    "\n",
    "# Run batch calls asynchronously\n",
    "async def main():\n",
    "    tasks = [call_llm(prompt) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    for result in results:\n",
    "        print(f\"Prompt: {result['prompt']}\")\n",
    "        if result.get(\"response\"):\n",
    "            print(f\"Response: {result['response']}\\n\")\n",
    "        else:\n",
    "            print(f\"Error: {result['error']}\\n\")\n",
    "    return results\n",
    "\n",
    "# Execute async batch calls in Jupyter\n",
    "results = asyncio.run(main())\n",
    "print(\"Batch results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab77aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 用中文讲一个幽默的笑话\n",
      "Response: 有一天，小明去买水果。他看到一个摊位上有个西瓜，特别大，心想：“这个西瓜肯定很甜！”于是他问老板：“这个西瓜多少钱？”\n",
      "\n",
      "老板说：“20块。”\n",
      "\n",
      "小明心里一惊：“这么贵啊！我能不能便宜点？”\n",
      "\n",
      "老板笑着说：“那你要不要试试？如果你觉得甜，我就给你打折。”\n",
      "\n",
      "小明点点头，老板切了一块给他。小明尝了一口，惊喜地说：“哇，真甜！”\n",
      "\n",
      "老板笑着说：“那你要不要买一个？”\n",
      "\n",
      "小明想了想，摇摇头：“算了，我还是买个便宜的苹果吧，西瓜太大了，放不下！”\n",
      "\n",
      "老板一愣：“你不是说它甜吗？”\n",
      "\n",
      "小明笑着说：“甜是甜，但我家冰箱不甜啊！”\n",
      "\n",
      "Prompt: 讲一个关于动物的幽默笑话\n",
      "Response: 有一天，一只乌龟走进酒吧，点了一杯酒。酒保看着它，问：“你怎么这么慢？”\n",
      "\n",
      "乌龟回答：“我在等我的朋友们，他们都在后面慢慢爬过来。”\n",
      "\n",
      "酒保好奇地问：“你朋友是谁？”\n",
      "\n",
      "乌龟自豪地说：“我有一只兔子朋友和一只蜗牛朋友！”\n",
      "\n",
      "酒保笑了：“那你们聚会的时候，谁最先到？”\n",
      "\n",
      "乌龟眨了眨眼：“当然是我，因为我总是提前到达，等他们的时候可以喝酒！”\n",
      "\n",
      "Prompt: 讲一个与程序员相关的幽默笑话\n",
      "Response: 当然可以！这里有一个与程序员相关的笑话：\n",
      "\n",
      "有一天，一个程序员走进酒吧，点了一杯啤酒。酒保问他：“你要什么样的啤酒？”\n",
      "\n",
      "程序员回答：“随便，反正我只会喝‘0’和‘1’！”\n",
      "\n",
      "酒保困惑地问：“那你喝得下去吗？”\n",
      "\n",
      "程序员笑着说：“当然，‘0’是空杯，‘1’是满杯！”\n",
      "\n",
      "希望这个笑话能让你开心！\n",
      "\n",
      "Prompt: 讲一个简短的幽默笑话\n",
      "Response: 有一天，小明问他的朋友：“你知道为什么海洋是蓝色的吗？”\n",
      "\n",
      "朋友摇摇头。\n",
      "\n",
      "小明得意地说：“因为鱼都在水里游泳，看到水面反射的天空，就觉得水是蓝色的！”\n",
      "\n",
      "朋友忍不住笑了：“那你怎么知道鱼不会觉得水是绿色的呢？”\n",
      "\n",
      "小明想了想，回答：“因为它们没有眼镜！”\n",
      "\n",
      "Batch results: [{'prompt': '用中文讲一个幽默的笑话', 'response': '有一天，小明去买水果。他看到一个摊位上有个西瓜，特别大，心想：“这个西瓜肯定很甜！”于是他问老板：“这个西瓜多少钱？”\\n\\n老板说：“20块。”\\n\\n小明心里一惊：“这么贵啊！我能不能便宜点？”\\n\\n老板笑着说：“那你要不要试试？如果你觉得甜，我就给你打折。”\\n\\n小明点点头，老板切了一块给他。小明尝了一口，惊喜地说：“哇，真甜！”\\n\\n老板笑着说：“那你要不要买一个？”\\n\\n小明想了想，摇摇头：“算了，我还是买个便宜的苹果吧，西瓜太大了，放不下！”\\n\\n老板一愣：“你不是说它甜吗？”\\n\\n小明笑着说：“甜是甜，但我家冰箱不甜啊！”'}, {'prompt': '讲一个关于动物的幽默笑话', 'response': '有一天，一只乌龟走进酒吧，点了一杯酒。酒保看着它，问：“你怎么这么慢？”\\n\\n乌龟回答：“我在等我的朋友们，他们都在后面慢慢爬过来。”\\n\\n酒保好奇地问：“你朋友是谁？”\\n\\n乌龟自豪地说：“我有一只兔子朋友和一只蜗牛朋友！”\\n\\n酒保笑了：“那你们聚会的时候，谁最先到？”\\n\\n乌龟眨了眨眼：“当然是我，因为我总是提前到达，等他们的时候可以喝酒！”'}, {'prompt': '讲一个与程序员相关的幽默笑话', 'response': '当然可以！这里有一个与程序员相关的笑话：\\n\\n有一天，一个程序员走进酒吧，点了一杯啤酒。酒保问他：“你要什么样的啤酒？”\\n\\n程序员回答：“随便，反正我只会喝‘0’和‘1’！”\\n\\n酒保困惑地问：“那你喝得下去吗？”\\n\\n程序员笑着说：“当然，‘0’是空杯，‘1’是满杯！”\\n\\n希望这个笑话能让你开心！'}, {'prompt': '讲一个简短的幽默笑话', 'response': '有一天，小明问他的朋友：“你知道为什么海洋是蓝色的吗？”\\n\\n朋友摇摇头。\\n\\n小明得意地说：“因为鱼都在水里游泳，看到水面反射的天空，就觉得水是蓝色的！”\\n\\n朋友忍不住笑了：“那你怎么知道鱼不会觉得水是绿色的呢？”\\n\\n小明想了想，回答：“因为它们没有眼镜！”'}]\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=GITHUB_TOKEN,\n",
    "    model=MODEL_ID,\n",
    "    base_url=BASE_URL,\n",
    "    temperature=0.2,\n",
    ")\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Define prompts for batch processing\n",
    "prompts = [\n",
    "   \"用中文讲一个幽默的笑话\",\n",
    "    \"讲一个关于动物的幽默笑话\",\n",
    "    \"讲一个与程序员相关的幽默笑话\",\n",
    "    \"讲一个简短的幽默笑话\",\n",
    "]\n",
    "\n",
    "# Asynchronous batch calling with LCEL\n",
    "async def main():\n",
    "    results = []\n",
    "    try:\n",
    "        responses = await chain.abatch([{\"input\": prompt} for prompt in prompts])\n",
    "        results = [{\"prompt\": prompt, \"response\": response} for prompt, response in zip(prompts, responses)]\n",
    "        for result in results:\n",
    "            print(f\"Prompt: {result['prompt']}\")\n",
    "            print(f\"Response: {result['response']}\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"Batch error:\", str(e))\n",
    "        results.append({\"prompt\": \"batch\", \"response\": None, \"error\": str(e)})\n",
    "    return results\n",
    "\n",
    "# Run async batch in Jupyter\n",
    "results = asyncio.run(main())\n",
    "print(\"Batch results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
